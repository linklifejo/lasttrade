
import os
import sqlite3
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import IterableDataset, DataLoader
from logger import logger
import math
import time

# [Safety Settings]
TARGET_RISE_RATE = 0.07  
WINDOW_SIZE = 60         
PREDICTION_HORIZON = 60  
BATCH_SIZE = 32
EPOCHS = 100
DB_PATH = "deep_learning.db"
MODEL_PATH = "DL_stock_model.pth"

# [ëª¨ë¸ êµ¬ì¡° ìœ ì§€ - Transformer]
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class HunterTransformer(nn.Module):
    def __init__(self, input_dim=5, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):
        super(HunterTransformer, self).__init__()
        self.input_embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        self.decoder = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())
        self.d_model = d_model

    def forward(self, src):
        src = self.input_embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = output[:, -1, :] 
        return self.decoder(output)

# [Memory Safe Dataset] ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
class StreamingStockDataset(IterableDataset):
    def __init__(self, db_path, table_name, window_size, horizon, target_rise, recent_days=1):
        self.db_path = db_path
        self.table_name = table_name
        self.window_size = window_size
        self.horizon = horizon
        self.target_rise = target_rise
        self.recent_days = recent_days # [New] ìµœê·¼ ë©°ì¹ ì¹˜ í•™ìŠµí• ì§€
        self.codes = self._get_codes()

    def _get_codes(self):
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            # [Optimization] ìµœê·¼ ë°ì´í„°ê°€ ìˆëŠ” ì¢…ëª©ë§Œ ì¡°íšŒí•˜ì—¬ ì†ë„ í–¥ìƒ
            # (sqliteì˜ date í•¨ìˆ˜ ì‚¬ìš©. ì‹¤í–‰ í™˜ê²½ì˜ ë¡œì»¬ íƒ€ì„ì¡´ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì—¬ìœ ìˆê²Œ)
            query = f"SELECT DISTINCT code FROM {self.table_name} WHERE timestamp >= date('now', '-{self.recent_days + 2} days')"
            cursor.execute(query)
            codes = [r[0] for r in cursor.fetchall()]
            conn.close()
            if not codes: 
                # ë§Œì•½ ìµœê·¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì „ì²´ ì¡°íšŒ (Fallback)
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute(f"SELECT DISTINCT code FROM {self.table_name}")
                codes = [r[0] for r in cursor.fetchall()]
                conn.close()
            return codes
        except:
            return []

    def _process_code_data(self, code):
        """ì¢…ëª© í•˜ë‚˜ì”© ì²˜ë¦¬ (ìµœê·¼ ë°ì´í„° ìœ„ì£¼)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 1. ë°ì´í„° ì¡°íšŒ (ë©”ëª¨ë¦¬ ë¶€ë‹´ ì ìŒ - ì¢…ëª© 1ê°œë‹ˆê¹Œ)
        # [Incremental Learning] ìµœê·¼ Nì¼ + ìœˆë„ìš° í™•ë³´ìš© ë²„í¼(2ì¼)
        query = f"SELECT * FROM {self.table_name} WHERE code = '{code}' AND timestamp >= date('now', '-{self.recent_days + 2} days') ORDER BY timestamp"
        try:
            cursor.execute(query)
            rows = cursor.fetchall()
            
            # ìˆ«ì íŒŒì‹±
            raw_data = []
            for r in rows:
                nums = [x for x in r if isinstance(x, (int, float))]
                if len(nums) >= 5: raw_data.append(nums[-5:])
            
            data = np.array(raw_data, dtype=np.float32)
            
            if len(data) < self.window_size + self.horizon:
                conn.close()
                return

            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± & Yield
            for i in range(0, len(data) - (self.window_size + self.horizon), 1): # Stride 1 (ê¼¼ê¼¼í•˜ê²Œ)
                current_close = data[i + self.window_size - 1][3]
                future_window = data[i + self.window_size : i + self.window_size + self.horizon]
                future_high = np.max(future_window[:, 1])
                
                max_return = (future_high - current_close) / (current_close + 1e-8)
                is_skyrocket = 1.0 if max_return >= self.target_rise else 0.0

                # [Under-Sampling] ì•ˆ ì˜¤ë¥¸ ë°ì´í„°ëŠ” í™•ë¥ ì ìœ¼ë¡œ ë²„ë¦¼ (ë©”ëª¨ë¦¬/ì‹œê°„ ì ˆì•½)
                if is_skyrocket == 0.0:
                    if np.random.rand() > 0.05: continue # 95% Drop

                # ì •ê·œí™”
                window_data = data[i:i+self.window_size]
                base = window_data[0] + 1e-8
                norm_window = (window_data / base) - 1.0
                
                yield torch.tensor(norm_window, dtype=torch.float32), torch.tensor([is_skyrocket], dtype=torch.float32)

        except Exception as e:
            # logger.error(f"Error processing {code}: {e}")
            pass
        finally:
            conn.close()

    def __iter__(self):
        # ì¢…ëª©ì„ í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©° ì œë„ˆë ˆì´í„° ì‹¤í–‰
        for code in self.codes:
            yield from self._process_code_data(code)

def get_table_name():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = [r[0] for r in cursor.fetchall()]
        conn.close()
        for t in ['ohlcv_1m', 'data_1m', 'candles_1m', 'candles', 'candle_history']:
            if t in tables: return t
        return tables[0] if tables else None
    except: return None

def train():
    logger.info("ğŸ”¥ [Safe Training] ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ ì‹œì‘ (Streaming)")
    table_name = get_table_name()
    if not table_name:
        logger.error("âŒ í…Œì´ë¸” ëª» ì°¾ìŒ")
        return

    # [Dataset & DataLoader]
    # num_workers=0 (DB ë™ì‹œì ‘ì† ì¶©ëŒ ë°©ì§€ ìœ„í•´ ì‹±ê¸€ í”„ë¡œì„¸ìŠ¤ ë¡œë”© ê¶Œì¥)
    dataset = StreamingStockDataset(DB_PATH, table_name, WINDOW_SIZE, PREDICTION_HORIZON, TARGET_RISE_RATE)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ’» Device: {device}")

    model = HunterTransformer(input_dim=5).to(device)
    
    # [ìˆ˜ì •] 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ì´ì–´ì„œ í•™ìŠµ)
    if os.path.exists(MODEL_PATH):
        try:
            # CPU/GPU í˜¸í™˜ì„± ê³ ë ¤í•˜ì—¬ map_location ì‚¬ìš©
            model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
            logger.info(f"ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {MODEL_PATH} (ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤)")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ ì‹œì‘): {e}")

    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    model.train()
    
    # [Epoch Loop]
    # IterableDatasetì€ len()ì´ ì—†ìœ¼ë¯€ë¡œ, ë°ì´í„°ê°€ ëë‚  ë•Œê¹Œì§€ ë•ë‹ˆë‹¤.
    for epoch in range(EPOCHS):
        total_loss = 0
        batch_count = 0
        start_time = time.time()
        
        for batch_X, batch_y in dataloader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            # [Log] ë„ˆë¬´ ì¡°ìš©í•˜ë©´ ë¶ˆì•ˆí•˜ë‹ˆê¹Œ 100ë°°ì¹˜ë§ˆë‹¤ ìƒì¡´ì‹ ê³ 
            if batch_count % 100 == 0:
                print(f"Epoch {epoch+1} | Batch {batch_count} | Loss: {loss.item():.4f}", end='\r')

        # ì—í¬í¬ ì¢…ë£Œ í›„ ì¤‘ê°„ ì €ì¥ (ë§¤ìš° ì¤‘ìš” - ì¤‘ê°„ì— ì£½ì–´ë„ ì‚´ë¦´ ìˆ˜ ìˆê²Œ)
        epoch_model_path = f"DL_model_epoch_{epoch+1}.pth"
        torch.save(model.state_dict(), epoch_model_path) # ì²´í¬í¬ì¸íŠ¸
        torch.save(model.state_dict(), MODEL_PATH)       # ë©”ì¸ íŒŒì¼ ê°±ì‹ 
        
        avg_loss = total_loss / (batch_count + 1e-8)
        elapsed = time.time() - start_time
        logger.info(f"âœ… Epoch {epoch+1}/{EPOCHS} ì™„ë£Œ | Loss: {avg_loss:.4f} | Time: {elapsed:.1f}s")
        
    logger.info("ğŸ‰ ëª¨ë“  í•™ìŠµ ì •ìƒ ì¢…ë£Œ!")

if __name__ == "__main__":
    train()
